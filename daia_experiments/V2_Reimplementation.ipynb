{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62dc4f35",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a601d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "from importlib import import_module\n",
    "from itertools import chain\n",
    "from os.path import join, exists\n",
    "from os import listdir, makedirs\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from shutil import rmtree\n",
    "from time import sleep\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a908f9",
   "metadata": {},
   "source": [
    "UTILS/UTIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f6c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_dir):\n",
    "    makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    logpath = join(log_dir, 'log.txt')\n",
    "    filemode = 'a' if exists(logpath) else 'w'\n",
    "\n",
    "    # set up logging to file - see previous section for more details\n",
    "    logging.basicConfig(level=logging.DEBUG,\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt='%m-%d %H:%M:%S',\n",
    "                        filename=logpath,\n",
    "                        filemode=filemode)\n",
    "    # define a Handler which writes INFO messages or higher to the sys.stderr\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.DEBUG)\n",
    "    # set a format which is simpler for console use\n",
    "    formatter = logging.Formatter('%(asctime)s: %(levelname)-8s %(message)s')\n",
    "    # tell the handler to use this format\n",
    "    console.setFormatter(formatter)\n",
    "    # add the handler to the root logger\n",
    "    logging.getLogger('').addHandler(console)\n",
    "\n",
    "\n",
    "def prepare_results_dir(config):\n",
    "    output_dir = join(config['results_root'], config['arch'],\n",
    "                      config['experiment_name'])\n",
    "    if config['clean_results_dir']:\n",
    "        if exists(output_dir):\n",
    "            print('Attention! Cleaning results directory in 10 seconds!')\n",
    "            sleep(10)\n",
    "        rmtree(output_dir, ignore_errors=True)\n",
    "    makedirs(output_dir, exist_ok=True)\n",
    "    makedirs(join(output_dir, 'weights'), exist_ok=True)\n",
    "    makedirs(join(output_dir, 'samples'), exist_ok=True)\n",
    "    makedirs(join(output_dir, 'results'), exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "def find_latest_epoch(dirpath):\n",
    "    # Files with weights are in format ddddd_{D,E,G}.pth\n",
    "    epoch_regex = re.compile(r'^(?P<n_epoch>\\d+)_[DEG]\\.pth$')\n",
    "    epochs_completed = []\n",
    "    if exists(join(dirpath, 'weights')):\n",
    "        dirpath = join(dirpath, 'weights')\n",
    "    for f in listdir(dirpath):\n",
    "        m = epoch_regex.match(f)\n",
    "        if m:\n",
    "            epochs_completed.append(int(m.group('n_epoch')))\n",
    "    return max(epochs_completed) if epochs_completed else 0\n",
    "\n",
    "\n",
    "def cuda_setup(cuda=True, gpu_idx=0):\n",
    "    if cuda and torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        torch.cuda.set_device(gpu_idx)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d79b5",
   "metadata": {},
   "source": [
    "plot and transform pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acfe860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Don't delete this line, even if PyCharm says it's an unused import.\n",
    "# It is required for projection='3d' in add_subplot()\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def rand_rotation_matrix(deflection=1.0, seed=None):\n",
    "    \"\"\"Creates a random rotation matrix.\n",
    "    Args:\n",
    "        deflection: the magnitude of the rotation. For 0, no rotation; for 1,\n",
    "                    completely random rotation. Small deflection => small\n",
    "                    perturbation.\n",
    "    DOI: http://www.realtimerendering.com/resources/GraphicsGems/gemsiii/rand_rotation.c\n",
    "         http://blog.lostinmyterminal.com/python/2015/05/12/random-rotation-matrix.html\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    theta, phi, z = np.random.uniform(size=(3,))\n",
    "\n",
    "    theta = theta * 2.0 * deflection * np.pi  # Rotation about the pole (Z).\n",
    "    phi = phi * 2.0 * np.pi  # For direction of pole deflection.\n",
    "    z = z * 2.0 * deflection  # For magnitude of pole deflection.\n",
    "\n",
    "    # Compute a vector V used for distributing points over the sphere\n",
    "    # via the reflection I - V Transpose(V).  This formulation of V\n",
    "    # will guarantee that if x[1] and x[2] are uniformly distributed,\n",
    "    # the reflected points will be uniform on the sphere.  Note that V\n",
    "    # has length sqrt(2) to eliminate the 2 in the Householder matrix.\n",
    "\n",
    "    r = np.sqrt(z)\n",
    "    V = (np.sin(phi) * r,\n",
    "         np.cos(phi) * r,\n",
    "         np.sqrt(2.0 - z))\n",
    "\n",
    "    st = np.sin(theta)\n",
    "    ct = np.cos(theta)\n",
    "\n",
    "    R = np.array(((ct, st, 0), (-st, ct, 0), (0, 0, 1)))\n",
    "\n",
    "    # Construct the rotation matrix  ( V Transpose(V) - I ) R.\n",
    "    M = (np.outer(V, V) - np.eye(3)).dot(R)\n",
    "    return M\n",
    "\n",
    "\n",
    "def add_gaussian_noise_to_pcloud(pcloud, mu=0, sigma=1):\n",
    "    gnoise = np.random.normal(mu, sigma, pcloud.shape[0])\n",
    "    gnoise = np.tile(gnoise, (3, 1)).T\n",
    "    pcloud += gnoise\n",
    "    return pcloud\n",
    "\n",
    "\n",
    "def add_rotation_to_pcloud(pcloud):\n",
    "    r_rotation = rand_rotation_matrix()\n",
    "\n",
    "    if len(pcloud.shape) == 2:\n",
    "        return pcloud.dot(r_rotation)\n",
    "    else:\n",
    "        return np.asarray([e.dot(r_rotation) for e in pcloud])\n",
    "\n",
    "\n",
    "def apply_augmentations(batch, conf):\n",
    "    if conf.gauss_augment is not None or conf.z_rotate:\n",
    "        batch = batch.copy()\n",
    "\n",
    "    if conf.gauss_augment is not None:\n",
    "        mu = conf.gauss_augment['mu']\n",
    "        sigma = conf.gauss_augment['sigma']\n",
    "        batch += np.random.normal(mu, sigma, batch.shape)\n",
    "\n",
    "    if conf.z_rotate:\n",
    "        r_rotation = rand_rotation_matrix()\n",
    "        r_rotation[0, 2] = 0\n",
    "        r_rotation[2, 0] = 0\n",
    "        r_rotation[1, 2] = 0\n",
    "        r_rotation[2, 1] = 0\n",
    "        r_rotation[2, 2] = 1\n",
    "        batch = batch.dot(r_rotation)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def unit_cube_grid_point_cloud(resolution, clip_sphere=False):\n",
    "    \"\"\"Returns the center coordinates of each cell of a 3D grid with\n",
    "    resolution^3 cells, that is placed in the unit-cube.\n",
    "    If clip_sphere it True it drops the \"corner\" cells that lie outside\n",
    "    the unit-sphere.\n",
    "    \"\"\"\n",
    "    grid = np.ndarray((resolution, resolution, resolution, 3), np.float32)\n",
    "    spacing = 1.0 / float(resolution - 1)\n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            for k in range(resolution):\n",
    "                grid[i, j, k, 0] = i * spacing - 0.5\n",
    "                grid[i, j, k, 1] = j * spacing - 0.5\n",
    "                grid[i, j, k, 2] = k * spacing - 0.5\n",
    "\n",
    "    if clip_sphere:\n",
    "        grid = grid.reshape(-1, 3)\n",
    "        grid = grid[norm(grid, axis=1) <= 0.5]\n",
    "\n",
    "    return grid, spacing\n",
    "\n",
    "\n",
    "def plot_3d_point_cloud(x, y, z, show=True, show_axis=True, in_u_sphere=False,\n",
    "                        marker='.', s=8, alpha=.8, figsize=(5, 5), elev=10,\n",
    "                        azim=240, axis=None, title=None, *args, **kwargs):\n",
    "    plt.switch_backend('agg')\n",
    "    if axis is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    else:\n",
    "        ax = axis\n",
    "        fig = axis\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    sc = ax.scatter(x, y, z, marker=marker, s=s, alpha=alpha, *args, **kwargs)\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    if in_u_sphere:\n",
    "        ax.set_xlim3d(-0.5, 0.5)\n",
    "        ax.set_ylim3d(-0.5, 0.5)\n",
    "        ax.set_zlim3d(-0.5, 0.5)\n",
    "    else:\n",
    "        # Multiply with 0.7 to squeeze free-space.\n",
    "        miv = 0.7 * np.min([np.min(x), np.min(y), np.min(z)])\n",
    "        mav = 0.7 * np.max([np.max(x), np.max(y), np.max(z)])\n",
    "        ax.set_xlim(miv, mav)\n",
    "        ax.set_ylim(miv, mav)\n",
    "        ax.set_zlim(miv, mav)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    if not show_axis:\n",
    "        plt.axis('off')\n",
    "\n",
    "    if 'c' in kwargs:\n",
    "        plt.colorbar(sc)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def transform_point_clouds(X, only_z_rotation=False, deflection=1.0):\n",
    "    r_rotation = rand_rotation_matrix(deflection)\n",
    "    if only_z_rotation:\n",
    "        r_rotation[0, 2] = 0\n",
    "        r_rotation[2, 0] = 0\n",
    "        r_rotation[1, 2] = 0\n",
    "        r_rotation[2, 1] = 0\n",
    "        r_rotation[2, 2] = 1\n",
    "    X = X.dot(r_rotation).astype(np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47879e",
   "metadata": {},
   "source": [
    "Weight InitializationM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adb0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname in ('Conv1d', 'Linear'):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d8e96",
   "metadata": {},
   "source": [
    "PLY Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f49375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Copyright 2014 Darsh Ranjan\n",
    "#\n",
    "#   This file is part of python-plyfile.\n",
    "#\n",
    "#   python-plyfile is free software: you can redistribute it and/or\n",
    "#   modify it under the terms of the GNU General Public License as\n",
    "#   published by the Free Software Foundation, either version 3 of the\n",
    "#   License, or (at your option) any later version.\n",
    "#\n",
    "#   python-plyfile is distributed in the hope that it will be useful,\n",
    "#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "#   General Public License for more details.\n",
    "#\n",
    "#   You should have received a copy of the GNU General Public License\n",
    "#   along with python-plyfile.  If not, see\n",
    "#       <http://www.gnu.org/licenses/>.\n",
    "\n",
    "from itertools import islice as _islice\n",
    "\n",
    "import numpy as _np\n",
    "from sys import byteorder as _byteorder\n",
    "\n",
    "\n",
    "try:\n",
    "    _range = xrange\n",
    "except NameError:\n",
    "    _range = range\n",
    "\n",
    "\n",
    "# Many-many relation\n",
    "_data_type_relation = [\n",
    "    ('int8', 'i1'),\n",
    "    ('char', 'i1'),\n",
    "    ('uint8', 'u1'),\n",
    "    ('uchar', 'b1'),\n",
    "    ('uchar', 'u1'),\n",
    "    ('int16', 'i2'),\n",
    "    ('short', 'i2'),\n",
    "    ('uint16', 'u2'),\n",
    "    ('ushort', 'u2'),\n",
    "    ('int32', 'i4'),\n",
    "    ('int', 'i4'),\n",
    "    ('uint32', 'u4'),\n",
    "    ('uint', 'u4'),\n",
    "    ('float32', 'f4'),\n",
    "    ('float', 'f4'),\n",
    "    ('float64', 'f8'),\n",
    "    ('double', 'f8')\n",
    "]\n",
    "\n",
    "_data_types = dict(_data_type_relation)\n",
    "_data_type_reverse = dict((b, a) for (a, b) in _data_type_relation)\n",
    "\n",
    "_types_list = []\n",
    "_types_set = set()\n",
    "for (_a, _b) in _data_type_relation:\n",
    "    if _a not in _types_set:\n",
    "        _types_list.append(_a)\n",
    "        _types_set.add(_a)\n",
    "    if _b not in _types_set:\n",
    "        _types_list.append(_b)\n",
    "        _types_set.add(_b)\n",
    "\n",
    "\n",
    "_byte_order_map = {\n",
    "    'ascii': '=',\n",
    "    'binary_little_endian': '<',\n",
    "    'binary_big_endian': '>'\n",
    "}\n",
    "\n",
    "_byte_order_reverse = {\n",
    "    '<': 'binary_little_endian',\n",
    "    '>': 'binary_big_endian'\n",
    "}\n",
    "\n",
    "_native_byte_order = {'little': '<', 'big': '>'}[_byteorder]\n",
    "\n",
    "\n",
    "def _lookup_type(type_str):\n",
    "    if type_str not in _data_type_reverse:\n",
    "        try:\n",
    "            type_str = _data_types[type_str]\n",
    "        except KeyError:\n",
    "            raise ValueError(\"field type %r not in %r\" %\n",
    "                             (type_str, _types_list))\n",
    "\n",
    "    return _data_type_reverse[type_str]\n",
    "\n",
    "\n",
    "def _split_line(line, n):\n",
    "    fields = line.split(None, n)\n",
    "    if len(fields) == n:\n",
    "        fields.append('')\n",
    "\n",
    "    assert len(fields) == n + 1\n",
    "\n",
    "    return fields\n",
    "\n",
    "\n",
    "def make2d(array, cols=None, dtype=None):\n",
    "    \"\"\"\n",
    "    Make a 2D array from an array of arrays.  The `cols' and `dtype'\n",
    "    arguments can be omitted if the array is not empty.\n",
    "    \"\"\"\n",
    "    if (cols is None or dtype is None) and not len(array):\n",
    "        raise RuntimeError(\"cols and dtype must be specified for empty \"\n",
    "                           \"array\")\n",
    "\n",
    "    if cols is None:\n",
    "        cols = len(array[0])\n",
    "\n",
    "    if dtype is None:\n",
    "        dtype = array[0].dtype\n",
    "\n",
    "    return _np.fromiter(array, [('_', dtype, (cols,))],\n",
    "                        count=len(array))['_']\n",
    "\n",
    "\n",
    "class PlyParseError(Exception):\n",
    "\n",
    "    \"\"\"\n",
    "    Raised when a PLY file cannot be parsed.\n",
    "    The attributes `element', `row', `property', and `message' give\n",
    "    additional information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, message, element=None, row=None, prop=None):\n",
    "        self.message = message\n",
    "        self.element = element\n",
    "        self.row = row\n",
    "        self.prop = prop\n",
    "\n",
    "        s = ''\n",
    "        if self.element:\n",
    "            s += 'element %r: ' % self.element.name\n",
    "        if self.row is not None:\n",
    "            s += 'row %d: ' % self.row\n",
    "        if self.prop:\n",
    "            s += 'property %r: ' % self.prop.name\n",
    "        s += self.message\n",
    "\n",
    "        Exception.__init__(self, s)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('PlyParseError(%r, element=%r, row=%r, prop=%r)' %\n",
    "                self.message, self.element, self.row, self.prop)\n",
    "\n",
    "\n",
    "class PlyData(object):\n",
    "\n",
    "    \"\"\"\n",
    "    PLY file header and data.\n",
    "    A PlyData instance is created in one of two ways: by the static\n",
    "    method PlyData.read (to read a PLY file), or directly from __init__\n",
    "    given a sequence of elements (which can then be written to a PLY\n",
    "    file).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, elements=[], text=False, byte_order='=',\n",
    "                 comments=[], obj_info=[]):\n",
    "        \"\"\"\n",
    "        elements: sequence of PlyElement instances.\n",
    "        text: whether the resulting PLY file will be text (True) or\n",
    "            binary (False).\n",
    "        byte_order: '<' for little-endian, '>' for big-endian, or '='\n",
    "            for native.  This is only relevant if `text' is False.\n",
    "        comments: sequence of strings that will be placed in the header\n",
    "            between the 'ply' and 'format ...' lines.\n",
    "        obj_info: like comments, but will be placed in the header with\n",
    "            \"obj_info ...\" instead of \"comment ...\".\n",
    "        \"\"\"\n",
    "        if byte_order == '=' and not text:\n",
    "            byte_order = _native_byte_order\n",
    "\n",
    "        self.byte_order = byte_order\n",
    "        self.text = text\n",
    "\n",
    "        self.comments = list(comments)\n",
    "        self.obj_info = list(obj_info)\n",
    "        self.elements = elements\n",
    "\n",
    "    def _get_elements(self):\n",
    "        return self._elements\n",
    "\n",
    "    def _set_elements(self, elements):\n",
    "        self._elements = tuple(elements)\n",
    "        self._index()\n",
    "\n",
    "    elements = property(_get_elements, _set_elements)\n",
    "\n",
    "    def _get_byte_order(self):\n",
    "        return self._byte_order\n",
    "\n",
    "    def _set_byte_order(self, byte_order):\n",
    "        if byte_order not in ['<', '>', '=']:\n",
    "            raise ValueError(\"byte order must be '<', '>', or '='\")\n",
    "\n",
    "        self._byte_order = byte_order\n",
    "\n",
    "    byte_order = property(_get_byte_order, _set_byte_order)\n",
    "\n",
    "    def _index(self):\n",
    "        self._element_lookup = dict((elt.name, elt) for elt in\n",
    "                                    self._elements)\n",
    "        if len(self._element_lookup) != len(self._elements):\n",
    "            raise ValueError(\"two elements with same name\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_header(stream):\n",
    "        \"\"\"\n",
    "        Parse a PLY header from a readable file-like stream.\n",
    "        \"\"\"\n",
    "       # print(stream)\n",
    "        lines = []\n",
    "        comments = {'comment': [], 'obj_info': []}\n",
    "        while True:\n",
    "            line = stream.readline().decode('ascii').strip()\n",
    "            fields = _split_line(line, 1)\n",
    "\n",
    "            if fields[0] == 'end_header':\n",
    "                break\n",
    "\n",
    "            elif fields[0] in comments.keys():\n",
    "                lines.append(fields)\n",
    "            else:\n",
    "                lines.append(line.split())\n",
    "\n",
    "        a = 0\n",
    "        if lines[a] != ['ply']:\n",
    "            raise PlyParseError(\"expected 'ply'\")\n",
    "\n",
    "        a += 1\n",
    "        while lines[a][0] in comments.keys():\n",
    "            comments[lines[a][0]].append(lines[a][1])\n",
    "            a += 1\n",
    "\n",
    "        if lines[a][0] != 'format':\n",
    "            raise PlyParseError(\"expected 'format'\")\n",
    "\n",
    "        if lines[a][2] != '1.0':\n",
    "            raise PlyParseError(\"expected version '1.0'\")\n",
    "\n",
    "        if len(lines[a]) != 3:\n",
    "            raise PlyParseError(\"too many fields after 'format'\")\n",
    "\n",
    "        fmt = lines[a][1]\n",
    "\n",
    "        if fmt not in _byte_order_map:\n",
    "            raise PlyParseError(\"don't understand format %r\" % fmt)\n",
    "\n",
    "        byte_order = _byte_order_map[fmt]\n",
    "        text = fmt == 'ascii'\n",
    "\n",
    "        a += 1\n",
    "        while a < len(lines) and lines[a][0] in comments.keys():\n",
    "            comments[lines[a][0]].append(lines[a][1])\n",
    "            a += 1\n",
    "\n",
    "        return PlyData(PlyElement._parse_multi(lines[a:]),\n",
    "                       text, byte_order,\n",
    "                       comments['comment'], comments['obj_info'])\n",
    "\n",
    "    @staticmethod\n",
    "    def read(stream):\n",
    "        \"\"\"\n",
    "        Read PLY data from a readable file-like object or filename.\n",
    "        \"\"\"\n",
    "        (must_close, stream) = _open_stream(stream, 'read')\n",
    "        try:\n",
    "            data = PlyData._parse_header(stream)\n",
    "            for elt in data:\n",
    "                elt._read(stream, data.text, data.byte_order)\n",
    "        finally:\n",
    "            if must_close:\n",
    "                stream.close()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def write(self, stream):\n",
    "        \"\"\"\n",
    "        Write PLY data to a writeable file-like object or filename.\n",
    "        \"\"\"\n",
    "        (must_close, stream) = _open_stream(stream, 'write')\n",
    "        try:\n",
    "            stream.write(self.header.encode('ascii'))\n",
    "            stream.write(b'\\r\\n')\n",
    "            for elt in self:\n",
    "                elt._write(stream, self.text, self.byte_order)\n",
    "        finally:\n",
    "            if must_close:\n",
    "                stream.close()\n",
    "\n",
    "    @property\n",
    "    def header(self):\n",
    "        \"\"\"\n",
    "        Provide PLY-formatted metadata for the instance.\n",
    "        \"\"\"\n",
    "        lines = ['ply']\n",
    "\n",
    "        if self.text:\n",
    "            lines.append('format ascii 1.0')\n",
    "        else:\n",
    "            lines.append('format ' +\n",
    "                         _byte_order_reverse[self.byte_order] +\n",
    "                         ' 1.0')\n",
    "\n",
    "        # Some information is lost here, since all comments are placed\n",
    "        # between the 'format' line and the first element.\n",
    "        for c in self.comments:\n",
    "            lines.append('comment ' + c)\n",
    "\n",
    "        for c in self.obj_info:\n",
    "            lines.append('obj_info ' + c)\n",
    "\n",
    "        lines.extend(elt.header for elt in self.elements)\n",
    "        lines.append('end_header')\n",
    "        return '\\r\\n'.join(lines)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.elements)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.elements)\n",
    "\n",
    "    def __contains__(self, name):\n",
    "        return name in self._element_lookup\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return self._element_lookup[name]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.header\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('PlyData(%r, text=%r, byte_order=%r, '\n",
    "                'comments=%r, obj_info=%r)' %\n",
    "                (self.elements, self.text, self.byte_order,\n",
    "                 self.comments, self.obj_info))\n",
    "\n",
    "\n",
    "def _open_stream(stream, read_or_write):\n",
    "    if hasattr(stream, read_or_write):\n",
    "        return (False, stream)\n",
    "    try:\n",
    "        return (True, open(stream, read_or_write[0] + 'b'))\n",
    "    except TypeError:\n",
    "        raise RuntimeError(\"expected open file or filename\")\n",
    "\n",
    "\n",
    "class PlyElement(object):\n",
    "\n",
    "    \"\"\"\n",
    "    PLY file element.\n",
    "    A client of this library doesn't normally need to instantiate this\n",
    "    directly, so the following is only for the sake of documenting the\n",
    "    internals.\n",
    "    Creating a PlyElement instance is generally done in one of two ways:\n",
    "    as a byproduct of PlyData.read (when reading a PLY file) and by\n",
    "    PlyElement.describe (before writing a PLY file).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, properties, count, comments=[]):\n",
    "        \"\"\"\n",
    "        This is not part of the public interface.  The preferred methods\n",
    "        of obtaining PlyElement instances are PlyData.read (to read from\n",
    "        a file) and PlyElement.describe (to construct from a numpy\n",
    "        array).\n",
    "        \"\"\"\n",
    "        self._name = str(name)\n",
    "        self._check_name()\n",
    "        self._count = count\n",
    "\n",
    "        self._properties = tuple(properties)\n",
    "        self._index()\n",
    "\n",
    "        self.comments = list(comments)\n",
    "\n",
    "        self._have_list = any(isinstance(p, PlyListProperty)\n",
    "                              for p in self.properties)\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        return self._count\n",
    "\n",
    "    def _get_data(self):\n",
    "        return self._data\n",
    "\n",
    "    def _set_data(self, data):\n",
    "        self._data = data\n",
    "        self._count = len(data)\n",
    "        self._check_sanity()\n",
    "\n",
    "    data = property(_get_data, _set_data)\n",
    "\n",
    "    def _check_sanity(self):\n",
    "        for prop in self.properties:\n",
    "            if prop.name not in self._data.dtype.fields:\n",
    "                raise ValueError(\"dangling property %r\" % prop.name)\n",
    "\n",
    "    def _get_properties(self):\n",
    "        return self._properties\n",
    "\n",
    "    def _set_properties(self, properties):\n",
    "        self._properties = tuple(properties)\n",
    "        self._check_sanity()\n",
    "        self._index()\n",
    "\n",
    "    properties = property(_get_properties, _set_properties)\n",
    "\n",
    "    def _index(self):\n",
    "        self._property_lookup = dict((prop.name, prop)\n",
    "                                     for prop in self._properties)\n",
    "        if len(self._property_lookup) != len(self._properties):\n",
    "            raise ValueError(\"two properties with same name\")\n",
    "\n",
    "    def ply_property(self, name):\n",
    "        return self._property_lookup[name]\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    def _check_name(self):\n",
    "        if any(c.isspace() for c in self._name):\n",
    "            msg = \"element name %r contains spaces\" % self._name\n",
    "            raise ValueError(msg)\n",
    "\n",
    "    def dtype(self, byte_order='='):\n",
    "        \"\"\"\n",
    "        Return the numpy dtype of the in-memory representation of the\n",
    "        data.  (If there are no list properties, and the PLY format is\n",
    "        binary, then this also accurately describes the on-disk\n",
    "        representation of the element.)\n",
    "        \"\"\"\n",
    "        return [(prop.name, prop.dtype(byte_order))\n",
    "                for prop in self.properties]\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_multi(header_lines):\n",
    "        \"\"\"\n",
    "        Parse a list of PLY element definitions.\n",
    "        \"\"\"\n",
    "        elements = []\n",
    "        while header_lines:\n",
    "            (elt, header_lines) = PlyElement._parse_one(header_lines)\n",
    "            elements.append(elt)\n",
    "\n",
    "        return elements\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_one(lines):\n",
    "        \"\"\"\n",
    "        Consume one element definition.  The unconsumed input is\n",
    "        returned along with a PlyElement instance.\n",
    "        \"\"\"\n",
    "        a = 0\n",
    "        line = lines[a]\n",
    "\n",
    "        if line[0] != 'element':\n",
    "            raise PlyParseError(\"expected 'element'\")\n",
    "        if len(line) > 3:\n",
    "            raise PlyParseError(\"too many fields after 'element'\")\n",
    "        if len(line) < 3:\n",
    "            raise PlyParseError(\"too few fields after 'element'\")\n",
    "\n",
    "        (name, count) = (line[1], int(line[2]))\n",
    "\n",
    "        comments = []\n",
    "        properties = []\n",
    "        while True:\n",
    "            a += 1\n",
    "            if a >= len(lines):\n",
    "                break\n",
    "\n",
    "            if lines[a][0] == 'comment':\n",
    "                comments.append(lines[a][1])\n",
    "            elif lines[a][0] == 'property':\n",
    "                properties.append(PlyProperty._parse_one(lines[a]))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return (PlyElement(name, properties, count, comments),\n",
    "                lines[a:])\n",
    "\n",
    "    @staticmethod\n",
    "    def describe(data, name, len_types={}, val_types={},\n",
    "                 comments=[]):\n",
    "        \"\"\"\n",
    "        Construct a PlyElement from an array's metadata.\n",
    "        len_types and val_types can be given as mappings from list\n",
    "        property names to type strings (like 'u1', 'f4', etc., or\n",
    "        'int8', 'float32', etc.). These can be used to define the length\n",
    "        and value types of list properties.  List property lengths\n",
    "        always default to type 'u1' (8-bit unsigned integer), and value\n",
    "        types default to 'i4' (32-bit integer).\n",
    "        \"\"\"\n",
    "        if not isinstance(data, _np.ndarray):\n",
    "            raise TypeError(\"only numpy arrays are supported\")\n",
    "\n",
    "        if len(data.shape) != 1:\n",
    "            raise ValueError(\"only one-dimensional arrays are \"\n",
    "                             \"supported\")\n",
    "\n",
    "        count = len(data)\n",
    "\n",
    "        properties = []\n",
    "        descr = data.dtype.descr\n",
    "\n",
    "        for t in descr:\n",
    "            if not isinstance(t[1], str):\n",
    "                raise ValueError(\"nested records not supported\")\n",
    "\n",
    "            if not t[0]:\n",
    "                raise ValueError(\"field with empty name\")\n",
    "\n",
    "            if len(t) != 2 or t[1][1] == 'O':\n",
    "                # non-scalar field, which corresponds to a list\n",
    "                # property in PLY.\n",
    "\n",
    "                if t[1][1] == 'O':\n",
    "                    if len(t) != 2:\n",
    "                        raise ValueError(\"non-scalar object fields not \"\n",
    "                                         \"supported\")\n",
    "\n",
    "                len_str = _data_type_reverse[len_types.get(t[0], 'u1')]\n",
    "                if t[1][1] == 'O':\n",
    "                    val_type = val_types.get(t[0], 'i4')\n",
    "                    val_str = _lookup_type(val_type)\n",
    "                else:\n",
    "                    val_str = _lookup_type(t[1][1:])\n",
    "\n",
    "                prop = PlyListProperty(t[0], len_str, val_str)\n",
    "            else:\n",
    "                val_str = _lookup_type(t[1][1:])\n",
    "                prop = PlyProperty(t[0], val_str)\n",
    "\n",
    "            properties.append(prop)\n",
    "\n",
    "        elt = PlyElement(name, properties, count, comments)\n",
    "        elt.data = data\n",
    "\n",
    "        return elt\n",
    "\n",
    "    def _read(self, stream, text, byte_order):\n",
    "        \"\"\"\n",
    "        Read the actual data from a PLY file.\n",
    "        \"\"\"\n",
    "        if text:\n",
    "            self._read_txt(stream)\n",
    "        else:\n",
    "            if self._have_list:\n",
    "                # There are list properties, so a simple load is\n",
    "                # impossible.\n",
    "                self._read_bin(stream, byte_order)\n",
    "            else:\n",
    "                # There are no list properties, so loading the data is\n",
    "                # much more straightforward.\n",
    "                self._data = _np.fromfile(stream,\n",
    "                                          self.dtype(byte_order),\n",
    "                                          self.count)\n",
    "\n",
    "        if len(self._data) < self.count:\n",
    "            k = len(self._data)\n",
    "            del self._data\n",
    "            raise PlyParseError(\"early end-of-file\", self, k)\n",
    "\n",
    "        self._check_sanity()\n",
    "\n",
    "    def _write(self, stream, text, byte_order):\n",
    "        \"\"\"\n",
    "        Write the data to a PLY file.\n",
    "        \"\"\"\n",
    "        if text:\n",
    "            self._write_txt(stream)\n",
    "        else:\n",
    "            if self._have_list:\n",
    "                # There are list properties, so serialization is\n",
    "                # slightly complicated.\n",
    "                self._write_bin(stream, byte_order)\n",
    "            else:\n",
    "                # no list properties, so serialization is\n",
    "                # straightforward.\n",
    "                self.data.astype(self.dtype(byte_order),\n",
    "                                 copy=False).tofile(stream)\n",
    "\n",
    "    def _read_txt(self, stream):\n",
    "        \"\"\"\n",
    "        Load a PLY element from an ASCII-format PLY file.  The element\n",
    "        may contain list properties.\n",
    "        \"\"\"\n",
    "        self._data = _np.empty(self.count, dtype=self.dtype())\n",
    "\n",
    "        k = 0\n",
    "        for line in _islice(iter(stream.readline, b''), self.count):\n",
    "            fields = iter(line.strip().split())\n",
    "            for prop in self.properties:\n",
    "                try:\n",
    "                    self._data[prop.name][k] = prop._from_fields(fields)\n",
    "                except StopIteration:\n",
    "                    raise PlyParseError(\"early end-of-line\",\n",
    "                                        self, k, prop)\n",
    "                except ValueError:\n",
    "                    raise PlyParseError(\"malformed input\",\n",
    "                                        self, k, prop)\n",
    "            try:\n",
    "                next(fields)\n",
    "            except StopIteration:\n",
    "                pass\n",
    "            else:\n",
    "                raise PlyParseError(\"expected end-of-line\", self, k)\n",
    "            k += 1\n",
    "\n",
    "        if k < self.count:\n",
    "            del self._data\n",
    "            raise PlyParseError(\"early end-of-file\", self, k)\n",
    "\n",
    "    def _write_txt(self, stream):\n",
    "        \"\"\"\n",
    "        Save a PLY element to an ASCII-format PLY file.  The element may\n",
    "        contain list properties.\n",
    "        \"\"\"\n",
    "        for rec in self.data:\n",
    "            fields = []\n",
    "            for prop in self.properties:\n",
    "                fields.extend(prop._to_fields(rec[prop.name]))\n",
    "\n",
    "            _np.savetxt(stream, [fields], '%.18g', newline='\\r\\n')\n",
    "\n",
    "    def _read_bin(self, stream, byte_order):\n",
    "        \"\"\"\n",
    "        Load a PLY element from a binary PLY file.  The element may\n",
    "        contain list properties.\n",
    "        \"\"\"\n",
    "        self._data = _np.empty(self.count, dtype=self.dtype(byte_order))\n",
    "\n",
    "        for k in _range(self.count):\n",
    "            for prop in self.properties:\n",
    "                try:\n",
    "                    self._data[prop.name][k] = \\\n",
    "                        prop._read_bin(stream, byte_order)\n",
    "                except StopIteration:\n",
    "                    raise PlyParseError(\"early end-of-file\",\n",
    "                                        self, k, prop)\n",
    "\n",
    "    def _write_bin(self, stream, byte_order):\n",
    "        \"\"\"\n",
    "        Save a PLY element to a binary PLY file.  The element may\n",
    "        contain list properties.\n",
    "        \"\"\"\n",
    "        for rec in self.data:\n",
    "            for prop in self.properties:\n",
    "                prop._write_bin(rec[prop.name], stream, byte_order)\n",
    "\n",
    "    @property\n",
    "    def header(self):\n",
    "        \"\"\"\n",
    "        Format this element's metadata as it would appear in a PLY\n",
    "        header.\n",
    "        \"\"\"\n",
    "        lines = ['element %s %d' % (self.name, self.count)]\n",
    "\n",
    "        # Some information is lost here, since all comments are placed\n",
    "        # between the 'element' line and the first property definition.\n",
    "        for c in self.comments:\n",
    "            lines.append('comment ' + c)\n",
    "\n",
    "        lines.extend(list(map(str, self.properties)))\n",
    "\n",
    "        return '\\r\\n'.join(lines)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.data[key] = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.header\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('PlyElement(%r, %r, count=%d, comments=%r)' %\n",
    "                (self.name, self.properties, self.count,\n",
    "                 self.comments))\n",
    "\n",
    "\n",
    "class PlyProperty(object):\n",
    "\n",
    "    \"\"\"\n",
    "    PLY property description.  This class is pure metadata; the data\n",
    "    itself is contained in PlyElement instances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, val_dtype):\n",
    "        self._name = str(name)\n",
    "        self._check_name()\n",
    "        self.val_dtype = val_dtype\n",
    "\n",
    "    def _get_val_dtype(self):\n",
    "        return self._val_dtype\n",
    "\n",
    "    def _set_val_dtype(self, val_dtype):\n",
    "        self._val_dtype = _data_types[_lookup_type(val_dtype)]\n",
    "\n",
    "    val_dtype = property(_get_val_dtype, _set_val_dtype)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    def _check_name(self):\n",
    "        if any(c.isspace() for c in self._name):\n",
    "            msg = \"Error: property name %r contains spaces\" % self._name\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_one(line):\n",
    "        assert line[0] == 'property'\n",
    "\n",
    "        if line[1] == 'list':\n",
    "            if len(line) > 5:\n",
    "                raise PlyParseError(\"too many fields after \"\n",
    "                                    \"'property list'\")\n",
    "            if len(line) < 5:\n",
    "                raise PlyParseError(\"too few fields after \"\n",
    "                                    \"'property list'\")\n",
    "\n",
    "            return PlyListProperty(line[4], line[2], line[3])\n",
    "\n",
    "        else:\n",
    "            if len(line) > 3:\n",
    "                raise PlyParseError(\"too many fields after \"\n",
    "                                    \"'property'\")\n",
    "            if len(line) < 3:\n",
    "                raise PlyParseError(\"too few fields after \"\n",
    "                                    \"'property'\")\n",
    "\n",
    "            return PlyProperty(line[2], line[1])\n",
    "\n",
    "    def dtype(self, byte_order='='):\n",
    "        \"\"\"\n",
    "        Return the numpy dtype description for this property (as a tuple\n",
    "        of strings).\n",
    "        \"\"\"\n",
    "        return byte_order + self.val_dtype\n",
    "\n",
    "    def _from_fields(self, fields):\n",
    "        \"\"\"\n",
    "        Parse from generator.  Raise StopIteration if the property could\n",
    "        not be read.\n",
    "        \"\"\"\n",
    "        return _np.dtype(self.dtype()).type(next(fields))\n",
    "\n",
    "    def _to_fields(self, data):\n",
    "        \"\"\"\n",
    "        Return generator over one item.\n",
    "        \"\"\"\n",
    "        yield _np.dtype(self.dtype()).type(data)\n",
    "\n",
    "    def _read_bin(self, stream, byte_order):\n",
    "        \"\"\"\n",
    "        Read data from a binary stream.  Raise StopIteration if the\n",
    "        property could not be read.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return _np.fromfile(stream, self.dtype(byte_order), 1)[0]\n",
    "        except IndexError:\n",
    "            raise StopIteration\n",
    "\n",
    "    def _write_bin(self, data, stream, byte_order):\n",
    "        \"\"\"\n",
    "        Write data to a binary stream.\n",
    "        \"\"\"\n",
    "        _np.dtype(self.dtype(byte_order)).type(data).tofile(stream)\n",
    "\n",
    "    def __str__(self):\n",
    "        val_str = _data_type_reverse[self.val_dtype]\n",
    "        return 'property %s %s' % (val_str, self.name)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'PlyProperty(%r, %r)' % (self.name,\n",
    "                                        _lookup_type(self.val_dtype))\n",
    "\n",
    "\n",
    "class PlyListProperty(PlyProperty):\n",
    "\n",
    "    \"\"\"\n",
    "    PLY list property description.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, len_dtype, val_dtype):\n",
    "        PlyProperty.__init__(self, name, val_dtype)\n",
    "\n",
    "        self.len_dtype = len_dtype\n",
    "\n",
    "    def _get_len_dtype(self):\n",
    "        return self._len_dtype\n",
    "\n",
    "    def _set_len_dtype(self, len_dtype):\n",
    "        self._len_dtype = _data_types[_lookup_type(len_dtype)]\n",
    "\n",
    "    len_dtype = property(_get_len_dtype, _set_len_dtype)\n",
    "\n",
    "    def dtype(self, byte_order='='):\n",
    "        \"\"\"\n",
    "        List properties always have a numpy dtype of \"object\".\n",
    "        \"\"\"\n",
    "        return '|O'\n",
    "\n",
    "    def list_dtype(self, byte_order='='):\n",
    "        \"\"\"\n",
    "        Return the pair (len_dtype, val_dtype) (both numpy-friendly\n",
    "        strings).\n",
    "        \"\"\"\n",
    "        return (byte_order + self.len_dtype,\n",
    "                byte_order + self.val_dtype)\n",
    "\n",
    "    def _from_fields(self, fields):\n",
    "        (len_t, val_t) = self.list_dtype()\n",
    "\n",
    "        n = int(_np.dtype(len_t).type(next(fields)))\n",
    "\n",
    "        data = _np.loadtxt(list(_islice(fields, n)), val_t, ndmin=1)\n",
    "        if len(data) < n:\n",
    "            raise StopIteration\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _to_fields(self, data):\n",
    "        \"\"\"\n",
    "        Return generator over the (numerical) PLY representation of the\n",
    "        list data (length followed by actual data).\n",
    "        \"\"\"\n",
    "        (len_t, val_t) = self.list_dtype()\n",
    "\n",
    "        data = _np.asarray(data, dtype=val_t).ravel()\n",
    "\n",
    "        yield _np.dtype(len_t).type(data.size)\n",
    "        for x in data:\n",
    "            yield x\n",
    "\n",
    "    def _read_bin(self, stream, byte_order):\n",
    "        (len_t, val_t) = self.list_dtype(byte_order)\n",
    "\n",
    "        try:\n",
    "            n = _np.fromfile(stream, len_t, 1)[0]\n",
    "        except IndexError:\n",
    "            raise StopIteration\n",
    "\n",
    "        data = _np.fromfile(stream, val_t, n)\n",
    "        if len(data) < n:\n",
    "            raise StopIteration\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _write_bin(self, data, stream, byte_order):\n",
    "        \"\"\"\n",
    "        Write data to a binary stream.\n",
    "        \"\"\"\n",
    "        (len_t, val_t) = self.list_dtype(byte_order)\n",
    "\n",
    "        data = _np.asarray(data, dtype=val_t).ravel()\n",
    "\n",
    "        _np.array(data.size, dtype=len_t).tofile(stream)\n",
    "        data.tofile(stream)\n",
    "\n",
    "    def __str__(self):\n",
    "        len_str = _data_type_reverse[self.len_dtype]\n",
    "        val_str = _data_type_reverse[self.val_dtype]\n",
    "        return 'property list %s %s %s' % (len_str, val_str, self.name)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('PlyListProperty(%r, %r, %r)' %\n",
    "                (self.name,\n",
    "                 _lookup_type(self.len_dtype),\n",
    "                 _lookup_type(self.val_dtype)))\n",
    "\n",
    "\n",
    "def load_ply(file_name: str,\n",
    "             with_faces: bool = False,\n",
    "             with_color: bool = False) -> _np.ndarray:\n",
    "   # print(file_name)\n",
    "    ply_data = PlyData.read(file_name)\n",
    "    points = ply_data['vertex']\n",
    "    points = _np.vstack([points['x'], points['y'], points['z']]).T\n",
    "    ret_val = [points]\n",
    "\n",
    "    if with_faces:\n",
    "        faces = _np.vstack(ply_data['face']['vertex_indices'])\n",
    "        ret_val.append(faces)\n",
    "\n",
    "    if with_color:\n",
    "        r = _np.vstack(ply_data['vertex']['red'])\n",
    "        g = _np.vstack(ply_data['vertex']['green'])\n",
    "        b = _np.vstack(ply_data['vertex']['blue'])\n",
    "        color = _np.hstack((r, g, b))\n",
    "        ret_val.append(color)\n",
    "\n",
    "    if len(ret_val) == 1:  # Unwrap the list\n",
    "        ret_val = ret_val[0]\n",
    "\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e6aab",
   "metadata": {},
   "source": [
    "ShapeNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c75335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import shutil\n",
    "from os import listdir, makedirs, remove\n",
    "from os.path import exists, join\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#from utils.plyfile import load_ply\n",
    "\n",
    "synth_id_to_category = {\n",
    "   # '02691156': 'airplane', \n",
    "    '02958343': 'car',# '04090263': 'rifle', # '04256520': 'sofa', \n",
    "   # '03001627': 'chair',\n",
    "    '04379243': 'table'\n",
    "#     '02801938': 'basket', '02773838': 'bag', \n",
    "#     '02808440': 'bathtub',   '02818832': 'bed',        '02828884': 'bench',\n",
    "#     '02834778': 'bicycle',   '02843684': 'birdhouse',  '02871439': 'bookshelf',\n",
    "#     '02876657': 'bottle',    '02880940': 'bowl',       '02924116': 'bus',\n",
    "#     '02933112': 'cabinet',   '02747177': 'can',        '02942699': 'camera',\n",
    "#     '02954340': 'cap',              ,\n",
    "#     '03046257': 'clock',     '03207941': 'dishwasher', '03211117': 'monitor',\n",
    "#     ,     '04401088': 'telephone',  '02946921': 'tin_can',\n",
    "#     '04460130': 'tower',     '04468005': 'train',      '03085013': 'keyboard',\n",
    "#     '03261776': 'earphone',  '03325088': 'faucet',     '03337140': 'file',\n",
    "#     '03467517': 'guitar',    '03513137': 'helmet',     '03593526': 'jar',\n",
    "#     '03624134': 'knife',     '03636649': 'lamp',       '03642806': 'laptop',\n",
    "#     '03691459': 'speaker',   '03710193': 'mailbox',    '03759954': 'microphone',\n",
    "#     '03761084': 'microwave', '03790512': 'motorcycle', '03797390': 'mug',\n",
    "#     '03928116': 'piano',     '03938244': 'pillow',     '03948459': 'pistol',\n",
    "#     '03991062': 'pot',       '04004475': 'printer',    '04074963': 'remote_control',\n",
    "#     '04090263': 'rifle',     '04099429': 'rocket',     '04225987': 'skateboard',\n",
    "#          '04330267': 'stove',      '04530566': 'vessel',\n",
    "#     '04554684': 'washer',    '02858304': 'boat',       '02992529': 'cellphone'\n",
    "}\n",
    "\n",
    "category_to_synth_id = {v: k for k, v in synth_id_to_category.items()}\n",
    "synth_id_to_number = {k: i for i, k in enumerate(synth_id_to_category.keys())}\n",
    "\n",
    "\n",
    "class ShapeNetDataset(Dataset):\n",
    "    def __init__(self, root_dir='3DAEE/shape_net_core_uniform_samples_2048', classes=[],\n",
    "                 transform=None, split='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the point clouds.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        self._maybe_download_data()\n",
    "\n",
    "        pc_df = self._get_names()\n",
    "        if classes:\n",
    "            if classes[0] not in synth_id_to_category.keys():\n",
    "                classes = [category_to_synth_id[c] for c in classes]\n",
    "            pc_df = pc_df[pc_df.category.isin(classes)].reset_index(drop=True)\n",
    "        else:\n",
    "            classes = synth_id_to_category.keys()\n",
    "\n",
    "        self.point_clouds_names_train = pd.concat([pc_df[pc_df['category'] == c][:int(0.85*len(pc_df[pc_df['category'] == c]))].reset_index(drop=True) for c in classes])\n",
    "        self.point_clouds_names_valid = pd.concat([pc_df[pc_df['category'] == c][int(0.85*len(pc_df[pc_df['category'] == c])):int(0.9*len(pc_df[pc_df['category'] == c]))].reset_index(drop=True) for c in classes])\n",
    "        self.point_clouds_names_test = pd.concat([pc_df[pc_df['category'] == c][int(0.9*len(pc_df[pc_df['category'] == c])):].reset_index(drop=True) for c in classes])\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == 'train':\n",
    "            pc_names = self.point_clouds_names_train\n",
    "        elif self.split == 'valid':\n",
    "            pc_names = self.point_clouds_names_valid\n",
    "        elif self.split == 'test':\n",
    "            pc_names = self.point_clouds_names_test\n",
    "        else:\n",
    "            raise ValueError('Invalid split. Should be train, valid or test.')\n",
    "        return len(pc_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'train':\n",
    "            pc_names = self.point_clouds_names_train\n",
    "        elif self.split == 'valid':\n",
    "            pc_names = self.point_clouds_names_valid\n",
    "        elif self.split == 'test':\n",
    "            pc_names = self.point_clouds_names_test\n",
    "        else:\n",
    "            raise ValueError('Invalid split. Should be train, valid or test.')\n",
    "\n",
    "        pc_category, pc_filename = pc_names.iloc[idx].values\n",
    "\n",
    "        pc_filepath = join(self.root_dir, pc_category, pc_filename)\n",
    "        sample = load_ply(pc_filepath)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, synth_id_to_number[pc_category]\n",
    "\n",
    "    def _get_names(self) -> pd.DataFrame:\n",
    "        filenames = []\n",
    "        for category_id in synth_id_to_category.keys():\n",
    "            for f in listdir(join(self.root_dir, category_id)):\n",
    "                if f not in ['.DS_Store']:\n",
    "                    filenames.append((category_id, f))\n",
    "        return pd.DataFrame(filenames, columns=['category', 'filename'])\n",
    "\n",
    "    def _maybe_download_data(self):\n",
    "        if exists(self.root_dir):\n",
    "            return\n",
    "        print(self.root_dir)\n",
    "        print(f'ShapeNet doesn\\'t exist in root directory {self.root_dir}. '\n",
    "              f'Downloading...')\n",
    "        makedirs(self.root_dir)\n",
    "\n",
    "        url = 'https://www.dropbox.com/s/vmsdrae6x5xws1v/shape_net_core_uniform_samples_2048.zip?dl=1'\n",
    "\n",
    "        data = urllib.request.urlopen(url)\n",
    "        filename = url.rpartition('/')[2][:-5]\n",
    "        file_path = join(self.root_dir, filename)\n",
    "        with open(file_path, mode='wb') as f:\n",
    "            d = data.read()\n",
    "            f.write(d)\n",
    "\n",
    "        print('Extracting...')\n",
    "        with ZipFile(file_path, mode='r') as zip_f:\n",
    "            zip_f.extractall(self.root_dir)\n",
    "\n",
    "        remove(file_path)\n",
    "\n",
    "        extracted_dir = join(self.root_dir,\n",
    "                             'shape_net_core_uniform_samples_2048')\n",
    "        for d in listdir(extracted_dir):\n",
    "            shutil.move(src=join(extracted_dir, d),\n",
    "                        dst=self.root_dir)\n",
    "\n",
    "        shutil.rmtree(extracted_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46894bd",
   "metadata": {},
   "source": [
    "Load Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba80239",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_507/2018643265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import cupy\n",
    "import torch\n",
    "from string import Template\n",
    "\n",
    "\n",
    "Stream = namedtuple('Stream', ['ptr'])\n",
    "\n",
    "\n",
    "def Dtype(t):\n",
    "    if isinstance(t, torch.cuda.FloatTensor):\n",
    "        return 'float'\n",
    "    elif isinstance(t, torch.cuda.DoubleTensor):\n",
    "        return 'double'\n",
    "\n",
    "\n",
    "@cupy.memoize(for_each_device=True)\n",
    "def load_kernel(kernel_name, code, **kwargs):\n",
    "    code = Template(code).substitute(**kwargs)\n",
    "    kernel_code = cupy.cuda.compile_with_cache(code)\n",
    "    return kernel_code.get_function(kernel_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d541b",
   "metadata": {},
   "source": [
    "Reconstruction Loss: Chamfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e64231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ChamferLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChamferLoss, self).__init__()\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    def forward(self, preds, gts):\n",
    "        P = self.batch_pairwise_dist(gts, preds)\n",
    "        mins, _ = torch.min(P, 1)\n",
    "        loss_1 = torch.sum(mins)\n",
    "        mins, _ = torch.min(P, 2)\n",
    "        loss_2 = torch.sum(mins)\n",
    "        return loss_1 + loss_2\n",
    "\n",
    "    def batch_pairwise_dist(self, x, y):\n",
    "        bs, num_points_x, points_dim = x.size()\n",
    "        _, num_points_y, _ = y.size()\n",
    "        xx = torch.bmm(x, x.transpose(2, 1))\n",
    "        yy = torch.bmm(y, y.transpose(2, 1))\n",
    "        zz = torch.bmm(x, y.transpose(2, 1))\n",
    "        if self.use_cuda:\n",
    "            dtype = torch.cuda.LongTensor\n",
    "        else:\n",
    "            dtype = torch.LongTensor\n",
    "        diag_ind_x = torch.arange(0, num_points_x).type(dtype)\n",
    "        diag_ind_y = torch.arange(0, num_points_y).type(dtype)\n",
    "        rx = xx[:, diag_ind_x, diag_ind_x].unsqueeze(1).expand_as(\n",
    "            zz.transpose(2, 1))\n",
    "        ry = yy[:, diag_ind_y, diag_ind_y].unsqueeze(1).expand_as(zz)\n",
    "        P = rx.transpose(2, 1) + ry - 2 * zz\n",
    "        return P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3daee0a",
   "metadata": {},
   "source": [
    "Generator, Encoder and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74aea094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_size = config['z_size']\n",
    "        self.use_bias = config['model']['G']['use_bias']\n",
    "        self.relu_slope = config['model']['G']['relu_slope']\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=self.z_size, out_features=64, bias=self.use_bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=64, out_features=128, bias=self.use_bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=128, out_features=512, bias=self.use_bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=512, out_features=1024, bias=self.use_bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=2048 * 3, bias=self.use_bias),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input.squeeze())\n",
    "        output = output.view(-1, 3, 2048)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_size = config['z_size']\n",
    "        self.use_bias = config['model']['D']['use_bias']\n",
    "        self.relu_slope = config['model']['D']['relu_slope']\n",
    "        self.dropout = config['model']['D']['dropout']\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "\n",
    "            nn.Linear(self.z_size, 512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(512, 512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(512, 128, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(64, 1, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logit = self.model(x)\n",
    "        return logit\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_size = config['z_size']\n",
    "        self.use_bias = config['model']['E']['use_bias']\n",
    "        self.relu_slope = config['model']['E']['relu_slope']\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=3, out_channels=64, kernel_size=1,\n",
    "                      bias=self.use_bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1,\n",
    "                      bias=self.use_bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=1,\n",
    "                      bias=self.use_bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(in_channels=256, out_channels=256, kernel_size=1,\n",
    "                      bias=self.use_bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=1,\n",
    "                      bias=self.use_bias),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256, bias=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.mu_layer = nn.Linear(256, self.z_size, bias=True)\n",
    "        self.std_layer = nn.Linear(256, self.z_size, bias=True)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv(x)\n",
    "        output2 = output.max(dim=2)[0]\n",
    "        logit = self.fc(output2)\n",
    "        mu = self.mu_layer(logit)\n",
    "        logvar = self.std_layer(logit)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929eb35f",
   "metadata": {},
   "source": [
    "Main Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8659be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    random.seed(config['seed'])\n",
    "    torch.manual_seed(config['seed'])\n",
    "    torch.cuda.manual_seed_all(config['seed'])\n",
    "\n",
    "    results_dir = prepare_results_dir(config)\n",
    "    starting_epoch = find_latest_epoch(results_dir) + 1\n",
    "\n",
    "    if not exists(join(results_dir, 'config.json')):\n",
    "        with open(join(results_dir, 'config.json'), mode='w') as f:\n",
    "            json.dump(config, f)\n",
    "\n",
    "    setup_logging(results_dir)\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    device = cuda_setup(config['cuda'], config['gpu'])\n",
    "    log.debug(f'Device variable: {device}')\n",
    "    if device.type == 'cuda':\n",
    "        log.debug(f'Current CUDA device: {torch.cuda.current_device()}')\n",
    "\n",
    "    weights_path = join(results_dir, 'weights')\n",
    "\n",
    "    #\n",
    "    # Dataset\n",
    "    #\n",
    "    dataset_name = config['dataset'].lower()\n",
    "    if dataset_name == 'shapenet':\n",
    "        #from datasets.shapenet import ShapeNetDataset\n",
    "        dataset = ShapeNetDataset(root_dir=config['data_dir'],\n",
    "                                  classes=config['classes'])\n",
    "    elif dataset_name == 'faust':\n",
    "        from datasets.dfaust import DFaustDataset\n",
    "        dataset = DFaustDataset(root_dir=config['data_dir'],\n",
    "                                classes=config['classes'])\n",
    "    else:\n",
    "        raise ValueError(f'Invalid dataset name. Expected `shapenet` or '\n",
    "                         f'`faust`. Got: `{dataset_name}`')\n",
    "    log.debug(\"Selected {} classes. Loaded {} samples.\".format(\n",
    "        'all' if not config['classes'] else ','.join(config['classes']),\n",
    "        len(dataset)))\n",
    "\n",
    "    points_dataloader = DataLoader(dataset, batch_size=50,\n",
    "                                   shuffle=True,num_workers=8,\n",
    "                                   drop_last=True, pin_memory=True)\n",
    "\n",
    "    #\n",
    "    # Models\n",
    "    #\n",
    "   # arch = import_module(\"3d-AAE/models/aae.py\")#f\"models.{config['arch']}\")\n",
    "    G = Generator(config).to(device)\n",
    "    E = Encoder(config).to(device)\n",
    "    D = Discriminator(config).to(device)\n",
    "\n",
    "    G.apply(weights_init)\n",
    "    E.apply(weights_init)\n",
    "    D.apply(weights_init)\n",
    "\n",
    "    if config['reconstruction_loss'].lower() == 'chamfer':\n",
    "       # from losses.champfer_loss import ChamferLoss\n",
    "        reconstruction_loss = ChamferLoss().to(device)\n",
    "    elif config['reconstruction_loss'].lower() == 'earth_mover':\n",
    "       # from losses.earth_mover_distance import EMD\n",
    "        reconstruction_loss = EMD().to(device)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid reconstruction loss. Accepted `chamfer` or '\n",
    "                         f'`earth_mover`, got: {config[\"reconstruction_loss\"]}')\n",
    "    #\n",
    "    # Float Tensors\n",
    "    #\n",
    "    fixed_noise = torch.FloatTensor(config['batch_size'], config['z_size'], 1)\n",
    "    fixed_noise.normal_(mean=config['normal_mu'], std=config['normal_std'])\n",
    "    noise = torch.FloatTensor(config['batch_size'], config['z_size'])\n",
    "\n",
    "    fixed_noise = fixed_noise.to(device)\n",
    "    noise = noise.to(device)\n",
    "\n",
    "    #\n",
    "    # Optimizers\n",
    "    #\n",
    "    EG_optim = getattr(optim, config['optimizer']['EG']['type'])\n",
    "    EG_optim = EG_optim(chain(E.parameters(), G.parameters()),\n",
    "                        **config['optimizer']['EG']['hyperparams'])\n",
    "\n",
    "    D_optim = getattr(optim, config['optimizer']['D']['type'])\n",
    "    D_optim = D_optim(D.parameters(),\n",
    "                      **config['optimizer']['D']['hyperparams'])\n",
    "\n",
    "    if starting_epoch > 1:\n",
    "        G.load_state_dict(torch.load(\n",
    "            join(weights_path, f'{starting_epoch-1:05}_G.pth')))\n",
    "        E.load_state_dict(torch.load(\n",
    "            join(weights_path, f'{starting_epoch-1:05}_E.pth')))\n",
    "        D.load_state_dict(torch.load(\n",
    "            join(weights_path, f'{starting_epoch-1:05}_D.pth')))\n",
    "\n",
    "        D_optim.load_state_dict(torch.load(\n",
    "            join(weights_path, f'{starting_epoch-1:05}_Do.pth')))\n",
    "\n",
    "        EG_optim.load_state_dict(torch.load(\n",
    "            join(weights_path, f'{starting_epoch-1:05}_EGo.pth')))\n",
    "\n",
    "    for epoch in range(starting_epoch, config['max_epochs'] + 1):\n",
    "       \n",
    "        start_epoch_time = datetime.now()\n",
    "\n",
    "        G.train()\n",
    "        E.train()\n",
    "        D.train()\n",
    "\n",
    "        total_loss_d = 0.0\n",
    "        total_loss_eg = 0.0\n",
    "        for i, point_data in enumerate(points_dataloader, 1):\n",
    "            \n",
    "            #print(\"yey\")\n",
    "            log.debug('-' * 20)\n",
    "\n",
    "            X, _ = point_data\n",
    "            X = X.to(device)\n",
    "\n",
    "            # Change dim [BATCH, N_POINTS, N_DIM] -> [BATCH, N_DIM, N_POINTS]\n",
    "            if X.size(-1) == 3:\n",
    "                X.transpose_(X.dim() - 2, X.dim() - 1)\n",
    "\n",
    "            codes, _, _ = E(X)\n",
    "            noise.normal_(mean=config['normal_mu'], std=config['normal_std'])\n",
    "            synth_logit = D(codes)\n",
    "            real_logit = D(noise)\n",
    "            loss_d = torch.mean(synth_logit) - torch.mean(real_logit)\n",
    "\n",
    "            alpha = torch.rand(config['batch_size'], 1).to(device)\n",
    "            differences = codes - noise\n",
    "            interpolates = noise + alpha * differences\n",
    "            disc_interpolates = D(interpolates)\n",
    "\n",
    "            gradients = grad(\n",
    "                outputs=disc_interpolates,\n",
    "                inputs=interpolates,\n",
    "                grad_outputs=torch.ones_like(disc_interpolates).to(device),\n",
    "                create_graph=True,\n",
    "                retain_graph=True,\n",
    "                only_inputs=True)[0]\n",
    "            slopes = torch.sqrt(torch.sum(gradients ** 2, dim=1))\n",
    "            gradient_penalty = ((slopes - 1) ** 2).mean()\n",
    "            loss_gp = config['gp_lambda'] * gradient_penalty\n",
    "            ###\n",
    "            loss_d += loss_gp\n",
    "\n",
    "            D_optim.zero_grad()\n",
    "            D.zero_grad()\n",
    "\n",
    "            loss_d.backward(retain_graph=True)\n",
    "            total_loss_d += loss_d.item()\n",
    "            D_optim.step()\n",
    "\n",
    "            # EG part of training\n",
    "            X_rec = G(codes)\n",
    "\n",
    "            loss_e = torch.mean(\n",
    "                config['reconstruction_coef'] *\n",
    "                reconstruction_loss(X.permute(0, 2, 1) + 0.5,\n",
    "                                    X_rec.permute(0, 2, 1) + 0.5))\n",
    "\n",
    "            synth_logit = D(codes)\n",
    "\n",
    "            loss_g = -torch.mean(synth_logit)\n",
    "\n",
    "            loss_eg = loss_e + loss_g\n",
    "            EG_optim.zero_grad()\n",
    "            E.zero_grad()\n",
    "            G.zero_grad()\n",
    "\n",
    "            loss_eg.backward()\n",
    "            total_loss_eg += loss_eg.item()\n",
    "            EG_optim.step()\n",
    "\n",
    "            log.debug(f'[{epoch}: ({i})] '\n",
    "                      f'Loss_D: {loss_d.item():.4f} '\n",
    "                      f'(GP: {loss_gp.item(): .4f}) '\n",
    "                      f'Loss_EG: {loss_eg.item():.4f} '\n",
    "                      f'(REC: {loss_e.item(): .4f}) '\n",
    "                      f'Time: {datetime.now() - start_epoch_time}')\n",
    "\n",
    "        log.debug(\n",
    "            f'[{epoch}/{config[\"max_epochs\"]}] '\n",
    "            f'Loss_D: {total_loss_d / i:.4f} '\n",
    "            f'Loss_EG: {total_loss_eg / i:.4f} '\n",
    "            f'Time: {datetime.now() - start_epoch_time}'\n",
    "        )\n",
    "\n",
    "        #\n",
    "        # Save intermediate results\n",
    "        #\n",
    "        G.eval()\n",
    "        E.eval()\n",
    "        D.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake = G(fixed_noise).data.cpu().numpy()\n",
    "            codes, _, _ = E(X)\n",
    "            X_rec = G(codes).data.cpu().numpy()\n",
    "        X=X.cpu().numpy()\n",
    "        for k in range(5):\n",
    "            fig = plot_3d_point_cloud(X[k][0], X[k][1], X[k][2],\n",
    "                                      in_u_sphere=True, show=False,\n",
    "                                      title=str(epoch))\n",
    "            fig.savefig(\n",
    "                join(results_dir, 'samples', f'{epoch:05}_{k}_real.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "        for k in range(5):\n",
    "            fig = plot_3d_point_cloud(fake[k][0], fake[k][1], fake[k][2],\n",
    "                                      in_u_sphere=True, show=False,\n",
    "                                      title=str(epoch))\n",
    "            fig.savefig(\n",
    "                join(results_dir, 'samples', f'{epoch:05}_{k}_fixed.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "        for k in range(5):\n",
    "            fig = plot_3d_point_cloud(X_rec[k][0],\n",
    "                                      X_rec[k][1],\n",
    "                                      X_rec[k][2],\n",
    "                                      in_u_sphere=True, show=False,\n",
    "                                      title=str(epoch))\n",
    "            fig.savefig(join(results_dir, 'samples',\n",
    "                             f'{epoch:05}_{k}_reconstructed.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "        if epoch % config['save_frequency'] == 0:\n",
    "            torch.save(G.state_dict(), join(weights_path, f'{epoch:05}_G.pth'))\n",
    "            torch.save(D.state_dict(), join(weights_path, f'{epoch:05}_D.pth'))\n",
    "            torch.save(E.state_dict(), join(weights_path, f'{epoch:05}_E.pth'))\n",
    "\n",
    "            torch.save(EG_optim.state_dict(),\n",
    "                       join(weights_path, f'{epoch:05}_EGo.pth'))\n",
    "\n",
    "            torch.save(D_optim.state_dict(),\n",
    "                       join(weights_path, f'{epoch:05}_Do.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a40373",
   "metadata": {},
   "source": [
    "Load Json and starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a01417fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '3d-AAE/settings/hyperparams.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_470/3132566561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#             config = json.load(f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#     assert config is not None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3d-AAE/settings/hyperparams.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '3d-AAE/settings/hyperparams.json'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    #reload(sys)\n",
    "    #sys.setdefaultencoding('utf-8')\n",
    "    logger = logging.getLogger()\n",
    "    #print(\"done 1\")\n",
    "#     parser = \"--\"\n",
    "#     parser.add_argument('-c', '--config', default=None, type=str,\n",
    "#                         help='config file path')\n",
    "#     args = parser.parse_args()\n",
    "   # print(\"done 2\")\n",
    "    config = None\n",
    "#     if args.config is not None and args.config.endswith('.json'):\n",
    "#         with open(args.config) as f:\n",
    "#             config = json.load(f)\n",
    "#     assert config is not None\n",
    "    with open(\"3d-AAE/settings/hyperparams.json\") as f:\n",
    "            config = json.load(f)\n",
    "    assert config is not None\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5a671",
   "metadata": {},
   "source": [
    "JSD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "387579d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "from scipy.stats import entropy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "__all__ = ['js_divercence_between_pc', 'jsd_between_point_cloud_sets']\n",
    "\n",
    "\n",
    "#\n",
    "# Compute JS divergence\n",
    "#\n",
    "\n",
    "\n",
    "def js_divercence_between_pc(pc1: torch.Tensor, pc2: torch.Tensor,\n",
    "                             voxels: int = 64) -> float:\n",
    "    \"\"\"Method for computing JSD from 2 sets of point clouds.\"\"\"\n",
    "    pc1_ = _pc_to_voxel_distribution(pc1, voxels)\n",
    "    pc2_ = _pc_to_voxel_distribution(pc2, voxels)\n",
    "    jsd = _js_divergence(pc1_, pc2_)\n",
    "    return jsd\n",
    "\n",
    "\n",
    "def _js_divergence(P, Q):\n",
    "    # Ensure probabilities.\n",
    "    P_ = P / np.sum(P)\n",
    "    Q_ = Q / np.sum(Q)\n",
    "\n",
    "    # Calculate JSD using scipy.stats.entropy()\n",
    "    e1 = entropy(P_, base=2)\n",
    "    e2 = entropy(Q_, base=2)\n",
    "    e_sum = entropy((P_ + Q_) / 2.0, base=2)\n",
    "    res1 = e_sum - ((e1 + e2) / 2.0)\n",
    "\n",
    "    # Calcujate JS-Div using manually defined KL divergence.\n",
    "    # res2 = _jsdiv(P_, Q_)\n",
    "    #\n",
    "    # if not np.allclose(res1, res2, atol=10e-5, rtol=0):\n",
    "    #     warnings.warn('Numerical values of two JSD methods don\\'t agree.')\n",
    "\n",
    "    return res1\n",
    "\n",
    "\n",
    "def _jsdiv(P, Q):\n",
    "    \"\"\"Another way of computing JSD to check numerical stability.\"\"\"\n",
    "    def _kldiv(A, B):\n",
    "        a = A.copy()\n",
    "        b = B.copy()\n",
    "        idx = np.logical_and(a > 0, b > 0)\n",
    "        a = a[idx]\n",
    "        b = b[idx]\n",
    "        return np.sum([v for v in a * np.log2(a / b)])\n",
    "\n",
    "    P_ = P / np.sum(P)\n",
    "    Q_ = Q / np.sum(Q)\n",
    "\n",
    "    M = 0.5 * (P_ + Q_)\n",
    "\n",
    "    return 0.5 * (_kldiv(P_, M) + _kldiv(Q_, M))\n",
    "\n",
    "\n",
    "def _pc_to_voxel_distribution(pc: torch.Tensor, n_voxels: int = 64) -> np.ndarray:\n",
    "    pc_ = pc.clamp(-0.5, 0.4999) + 0.5\n",
    "    # Because points are in range [0, 1], simple multiplication will bin them.\n",
    "    pc_ = (pc_ * n_voxels).int()\n",
    "    pc_ = pc_[:, :, 0] * n_voxels ** 2 + pc_[:, :, 1] * n_voxels + pc_[:, :, 2]\n",
    "\n",
    "    B = np.zeros(n_voxels**3, dtype=np.int32)\n",
    "    values, amounts = np.unique(pc_, return_counts=True)\n",
    "    B[values] = amounts\n",
    "    return B\n",
    "\n",
    "\n",
    "#\n",
    "# Stanford way to calculate JSD\n",
    "#\n",
    "\n",
    "\n",
    "def jsd_between_point_cloud_sets(sample_pcs, ref_pcs, voxels=28,\n",
    "                                 in_unit_sphere=True):\n",
    "    \"\"\"Computes the JSD between two sets of point-clouds, as introduced in the\n",
    "    paper ```Learning Representations And Generative Models For 3D Point\n",
    "    Clouds```.\n",
    "    Args:\n",
    "        sample_pcs: (np.ndarray S1xR2x3) S1 point-clouds, each of R1 points.\n",
    "        ref_pcs: (np.ndarray S2xR2x3) S2 point-clouds, each of R2 points.\n",
    "        voxels: (int) grid-resolution. Affects granularity of measurements.\n",
    "    \"\"\"\n",
    "    sample_grid_var = _entropy_of_occupancy_grid(sample_pcs, voxels,\n",
    "                                                 in_unit_sphere)[1]\n",
    "    ref_grid_var = _entropy_of_occupancy_grid(ref_pcs, voxels,\n",
    "                                              in_unit_sphere)[1]\n",
    "    return _js_divergence(sample_grid_var, ref_grid_var)\n",
    "\n",
    "\n",
    "def _entropy_of_occupancy_grid(pclouds, grid_resolution, in_sphere=False):\n",
    "    \"\"\"Given a collection of point-clouds, estimate the entropy of the random\n",
    "    variables corresponding to occupancy-grid activation patterns.\n",
    "    Inputs:\n",
    "        pclouds: (numpy array) #point-clouds x points per point-cloud x 3\n",
    "        grid_resolution (int) size of occupancy grid that will be used.\n",
    "    \"\"\"\n",
    "    pclouds = pclouds.cpu().numpy()\n",
    "    epsilon = 10e-4\n",
    "    bound = 0.5 + epsilon\n",
    "    # if abs(np.max(pclouds)) > bound or abs(np.min(pclouds)) > bound:\n",
    "    #     warnings.warn('Point-clouds are not in unit cube.')\n",
    "    #\n",
    "    # if in_sphere and np.max(np.sqrt(np.sum(pclouds ** 2, axis=2))) > bound:\n",
    "    #     warnings.warn('Point-clouds are not in unit sphere.')\n",
    "\n",
    "    grid_coordinates, _ = _unit_cube_grid_point_cloud(grid_resolution, in_sphere)\n",
    "    grid_coordinates = grid_coordinates.reshape(-1, 3)\n",
    "    grid_counters = np.zeros(len(grid_coordinates))\n",
    "    grid_bernoulli_rvars = np.zeros(len(grid_coordinates))\n",
    "    nn = NearestNeighbors(n_neighbors=1).fit(grid_coordinates)\n",
    "\n",
    "    for pc in pclouds:\n",
    "        _, indices = nn.kneighbors(pc)\n",
    "        indices = np.squeeze(indices)\n",
    "        for i in indices:\n",
    "            grid_counters[i] += 1\n",
    "        indices = np.unique(indices)\n",
    "        for i in indices:\n",
    "            grid_bernoulli_rvars[i] += 1\n",
    "\n",
    "    acc_entropy = 0.0\n",
    "    n = float(len(pclouds))\n",
    "    for g in grid_bernoulli_rvars:\n",
    "        p = 0.0\n",
    "        if g > 0:\n",
    "            p = float(g) / n\n",
    "            acc_entropy += entropy([p, 1.0 - p])\n",
    "\n",
    "    return acc_entropy / len(grid_counters), grid_counters\n",
    "\n",
    "\n",
    "def _unit_cube_grid_point_cloud(resolution, clip_sphere=False):\n",
    "    \"\"\"Returns the center coordinates of each cell of a 3D grid with resolution^3 cells,\n",
    "    that is placed in the unit-cube.\n",
    "    If clip_sphere it True it drops the \"corner\" cells that lie outside the unit-sphere.\n",
    "    \"\"\"\n",
    "    grid = np.ndarray((resolution, resolution, resolution, 3), np.float32)\n",
    "    spacing = 1.0 / float(resolution - 1)\n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            for k in range(resolution):\n",
    "                grid[i, j, k, 0] = i * spacing - 0.5\n",
    "                grid[i, j, k, 1] = j * spacing - 0.5\n",
    "                grid[i, j, k, 2] = k * spacing - 0.5\n",
    "\n",
    "    if clip_sphere:\n",
    "        grid = grid.reshape(-1, 3)\n",
    "        grid = grid[norm(grid, axis=1) <= 0.5]\n",
    "\n",
    "    return grid, spacing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d1850",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db5f6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from importlib import import_module\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.distributions.beta import Beta\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from datasets.shapenet import ShapeNetDataset\n",
    "# from metrics.jsd import jsd_between_point_cloud_sets\n",
    "# from utils.util import cuda_setup, setup_logging\n",
    "\n",
    "\n",
    "def _get_epochs_by_regex(path, regex):\n",
    "    reg = re.compile(regex)\n",
    "    return {int(w[:5]) for w in listdir(path) if reg.match(w)}\n",
    "\n",
    "\n",
    "def main(eval_config):\n",
    "    # Load hyperparameters as they were during training\n",
    "    train_results_path = join(eval_config['results_root'], eval_config['arch'],\n",
    "                              eval_config['experiment_name'])\n",
    "    with open(join(train_results_path, 'config.json')) as f:\n",
    "        train_config = json.load(f)\n",
    "\n",
    "    random.seed(train_config['seed'])\n",
    "    torch.manual_seed(train_config['seed'])\n",
    "    torch.cuda.manual_seed_all(train_config['seed'])\n",
    "\n",
    "    setup_logging(join(train_results_path, 'results'))\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    log.debug('Evaluating JensenShannon divergences on validation set on all '\n",
    "              'saved epochs.')\n",
    "\n",
    "    weights_path = join(train_results_path, 'weights')\n",
    "\n",
    "    # Find all epochs that have saved model weights\n",
    "    e_epochs = _get_epochs_by_regex(weights_path, r'(?P<epoch>\\d{5})_E\\.pth')\n",
    "    g_epochs = _get_epochs_by_regex(weights_path, r'(?P<epoch>\\d{5})_G\\.pth')\n",
    "    epochs = sorted(e_epochs.intersection(g_epochs))\n",
    "    log.debug(f'Testing epochs: {epochs}')\n",
    "\n",
    "    device = cuda_setup(eval_config['cuda'], eval_config['gpu'])\n",
    "    log.debug(f'Device variable: {device}')\n",
    "    if device.type == 'cuda':\n",
    "        log.debug(f'Current CUDA device: {torch.cuda.current_device()}')\n",
    "\n",
    "    #\n",
    "    # Dataset\n",
    "    #\n",
    "    dataset_name = train_config['dataset'].lower()\n",
    "    if dataset_name == 'shapenet':\n",
    "        dataset = ShapeNetDataset(root_dir=\"shape_net_core_uniform_samples_2048\",\n",
    "                                  classes=train_config['classes'], split='valid')\n",
    "    elif dataset_name == 'faust':\n",
    "        from datasets.dfaust import DFaustDataset\n",
    "        dataset = DFaustDataset(root_dir=train_config['data_dir'],\n",
    "                                classes=train_config['classes'], split='valid')\n",
    "    elif dataset_name == 'mcgill':\n",
    "        from datasets.mcgill import McGillDataset\n",
    "        dataset = McGillDataset(root_dir=train_config['data_dir'],\n",
    "                                classes=train_config['classes'], split='valid')\n",
    "    else:\n",
    "        raise ValueError(f'Invalid dataset name. Expected `shapenet` or '\n",
    "                         f'`faust`. Got: `{dataset_name}`')\n",
    "    classes_selected = ('all' if not train_config['classes']\n",
    "                        else ','.join(train_config['classes']))\n",
    "    log.debug(f'Selected {classes_selected} classes. Loaded {len(dataset)} '\n",
    "              f'samples.')\n",
    "\n",
    "    if 'distribution' in train_config:\n",
    "        distribution = train_config['distribution']\n",
    "    elif 'distribution' in eval_config:\n",
    "        distribution = eval_config['distribution']\n",
    "    else:\n",
    "        log.warning('No distribution type specified. Assumed normal = N(0, 0.2)')\n",
    "        distribution = 'normal'\n",
    "\n",
    "    #\n",
    "    # Models\n",
    "    #\n",
    "    #arch = import_module(f\"model.architectures.{train_config['arch']}\")\n",
    "    E = Encoder(train_config).to(device)\n",
    "    G = Generator(train_config).to(device)\n",
    "\n",
    "    E.eval()\n",
    "    G.eval()\n",
    "\n",
    "    num_samples = len(dataset.point_clouds_names_valid)\n",
    "    data_loader = DataLoader(dataset, batch_size=num_samples,\n",
    "                             shuffle=False, num_workers=4,\n",
    "                             drop_last=False, pin_memory=True)\n",
    "\n",
    "    # We take 3 times as many samples as there are in test data in order to\n",
    "    # perform JSD calculation in the same manner as in the reference publication\n",
    "    noise = torch.FloatTensor(3 * num_samples, train_config['z_size'], 1)\n",
    "    noise = noise.to(device)\n",
    "\n",
    "    X, _ = next(iter(data_loader))\n",
    "    X = X.to(device)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for epoch in reversed(epochs):\n",
    "        try:\n",
    "            E.load_state_dict(torch.load(\n",
    "                join(weights_path, f'{epoch:05}_E.pth')))\n",
    "            G.load_state_dict(torch.load(\n",
    "                join(weights_path, f'{epoch:05}_G.pth')))\n",
    "\n",
    "            start_clock = datetime.now()\n",
    "\n",
    "            # We average JSD computation from 3 independet trials.\n",
    "            js_results = []\n",
    "            for _ in range(3):\n",
    "                if distribution == 'normal':\n",
    "                    noise.normal_(0, 0.2)\n",
    "                elif distribution == 'beta':\n",
    "                    noise_np = np.random.beta(train_config['z_beta_a'],\n",
    "                                              train_config['z_beta_b'],\n",
    "                                              noise.shape)\n",
    "                    noise = torch.tensor(noise_np).float().round().to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    X_g = G(noise)\n",
    "                if X_g.shape[-2:] == (3, 2048):\n",
    "                    X_g.transpose_(1, 2)\n",
    "\n",
    "                jsd = jsd_between_point_cloud_sets(X, X_g, voxels=28)\n",
    "                #js_divercence_between_pc(X, X_g, voxels=64)\n",
    "                js_results.append(jsd)\n",
    "\n",
    "            js_result = np.mean(js_results)\n",
    "            log.debug(f'Epoch: {epoch} JSD: {js_result: .6f} '\n",
    "                      f'Time: {datetime.now() - start_clock}')\n",
    "            results[epoch] = js_result\n",
    "        except KeyboardInterrupt:\n",
    "            log.debug(f'Interrupted during epoch: {epoch}')\n",
    "            break\n",
    "\n",
    "    results = pd.DataFrame.from_dict(results, orient='index', columns=['jsd'])\n",
    "    log.debug(f\"Minimum JSD at epoch {results.idxmin()['jsd']}: \"\n",
    "              f\"{results.min()['jsd']: .6f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38009eec-bc0c-48b7-a7eb-cc734b10f38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/3DAEE'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52565666",
   "metadata": {},
   "source": [
    "Load JSON and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa04a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 17:54:31,809: DEBUG    Evaluating JensenShannon divergences on validation set on all saved epochs.\n",
      "2021-08-11 17:54:31,809: DEBUG    Evaluating JensenShannon divergences on validation set on all saved epochs.\n",
      "2021-08-11 17:54:31,809: DEBUG    Evaluating JensenShannon divergences on validation set on all saved epochs.\n",
      "2021-08-11 17:54:31,814: DEBUG    Testing epochs: [910]\n",
      "2021-08-11 17:54:31,814: DEBUG    Testing epochs: [910]\n",
      "2021-08-11 17:54:31,814: DEBUG    Testing epochs: [910]\n",
      "2021-08-11 17:54:31,816: DEBUG    Device variable: cuda\n",
      "2021-08-11 17:54:31,816: DEBUG    Device variable: cuda\n",
      "2021-08-11 17:54:31,816: DEBUG    Device variable: cuda\n",
      "2021-08-11 17:54:31,819: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 17:54:31,819: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 17:54:31,819: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 17:54:31,879: DEBUG    Selected all classes. Loaded 801 samples.\n",
      "2021-08-11 17:54:31,879: DEBUG    Selected all classes. Loaded 801 samples.\n",
      "2021-08-11 17:54:31,879: DEBUG    Selected all classes. Loaded 801 samples.\n",
      "2021-08-11 17:55:25,594: DEBUG    Epoch: 910 JSD:  0.307732 Time: 0:00:52.692432\n",
      "2021-08-11 17:55:25,594: DEBUG    Epoch: 910 JSD:  0.307732 Time: 0:00:52.692432\n",
      "2021-08-11 17:55:25,594: DEBUG    Epoch: 910 JSD:  0.307732 Time: 0:00:52.692432\n",
      "2021-08-11 17:55:25,606: DEBUG    Minimum JSD at epoch 910:  0.307732\n",
      "2021-08-11 17:55:25,606: DEBUG    Minimum JSD at epoch 910:  0.307732\n",
      "2021-08-11 17:55:25,606: DEBUG    Minimum JSD at epoch 910:  0.307732\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-c', '--config', default=None, type=str,\n",
    "#                         help='File path for evaluation config')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    evaluation_config = None\n",
    "   # if args.config is not None and args.config.endswith('.json'):\n",
    "    with open(\"hyperparams.json\") as f:\n",
    "        evaluation_config = json.load(f)\n",
    "    assert evaluation_config is not None\n",
    "\n",
    "    main(evaluation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80237c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/3DAEE'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82488dec-2e38-415c-ab0a-737998397d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from importlib import import_module\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Beta\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from datasets.shapenet.shapenet import ShapeNetDataset\n",
    "# from loggers.basic_logger import setup_logging\n",
    "# from utils.util import find_latest_epoch, cuda_setup\n",
    "\n",
    "\n",
    "def main(eval_config):\n",
    "    # Load hyperparameters as they were during training\n",
    "    train_results_path = join(eval_config['results_root'], eval_config['arch'],\n",
    "                              eval_config['experiment_name'])\n",
    "    with open(join(train_results_path, 'config.json')) as f:\n",
    "        train_config = json.load(f)\n",
    "\n",
    "    random.seed(train_config['seed'])\n",
    "    torch.manual_seed(train_config['seed'])\n",
    "    torch.cuda.manual_seed_all(train_config['seed'])\n",
    "\n",
    "    setup_logging(join(train_results_path, 'results'))\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    weights_path = join(train_results_path, 'weights')\n",
    "    if eval_config['epoch'] == 0:\n",
    "        epoch = find_latest_epoch(weights_path)\n",
    "    else:\n",
    "        epoch = eval_config['epoch']\n",
    "    log.debug(f'Starting from epoch: {epoch}')\n",
    "\n",
    "    device = cuda_setup(eval_config['cuda'], eval_config['gpu'])\n",
    "    log.debug(f'Device variable: {device}')\n",
    "    if device.type == 'cuda':\n",
    "        log.debug(f'Current CUDA device: {torch.cuda.current_device()}')\n",
    "\n",
    "    #\n",
    "    # Dataset\n",
    "    #\n",
    "    dataset_name = train_config['dataset'].lower()\n",
    "    if dataset_name == 'shapenet':\n",
    "        dataset = ShapeNetDataset(root_dir=\"shape_net_core_uniform_samples_2048\",\n",
    "                                  classes=train_config['classes'], split='test')\n",
    "    elif dataset_name == 'faust':\n",
    "        from datasets.dfaust import DFaustDataset\n",
    "        dataset = DFaustDataset(root_dir=train_config['data_dir'],\n",
    "                                classes=train_config['classes'], split='test')\n",
    "    elif dataset_name == 'mcgill':\n",
    "        from datasets.mcgill import McGillDataset\n",
    "        dataset = McGillDataset(root_dir=train_config['data_dir'],\n",
    "                                classes=train_config['classes'], split='test')\n",
    "    else:\n",
    "        raise ValueError(f'Invalid dataset name. Expected `shapenet` or '\n",
    "                         f'`faust`. Got: `{dataset_name}`')\n",
    "    classes_selected = ('all' if not train_config['classes']\n",
    "                        else ','.join(train_config['classes']))\n",
    "    log.debug(f'Selected {classes_selected} classes. Loaded {len(dataset)} '\n",
    "              f'samples.')\n",
    "\n",
    "    if 'distribution' in train_config:\n",
    "        distribution = train_config['distribution']\n",
    "    elif 'distribution' in eval_config:\n",
    "        distribution = eval_config['distribution']\n",
    "    else:\n",
    "        log.warning('No distribution type specified. Assumed normal = N(0, 0.2)')\n",
    "        distribution = 'normal'\n",
    "\n",
    "    #\n",
    "    # Models\n",
    "    #\n",
    "    #arch = import_module(f\"model.architectures.{eval_config['arch']}\")\n",
    "    E = Encoder(train_config).to(device)\n",
    "    G = Generator(train_config).to(device)\n",
    "\n",
    "    #\n",
    "    # Load saved state\n",
    "    #\n",
    "    E.load_state_dict(torch.load(join(weights_path, f'{epoch:05}_E.pth')))\n",
    "    G.load_state_dict(torch.load(join(weights_path, f'{epoch:05}_G.pth')))\n",
    "\n",
    "    E.eval()\n",
    "    G.eval()\n",
    "\n",
    "    num_samples = len(dataset.point_clouds_names_test)\n",
    "    data_loader = DataLoader(dataset, batch_size=16,\n",
    "                             shuffle=False, num_workers=1,\n",
    "                             drop_last=False, pin_memory=True)\n",
    "\n",
    "    # We take 3 times as many samples as there are in test data in order to\n",
    "    # perform JSD calculation in the same manner as in the reference publication\n",
    "    noise = torch.FloatTensor(3 * num_samples, train_config['z_size'], 1)\n",
    "    noise = noise.to(device)\n",
    "\n",
    "    X, _ = next(iter(data_loader))\n",
    "    X = X.to(device)\n",
    "    X1=X.cpu()\n",
    "    np.save(join(train_results_path, 'results', f'{epoch:05}_X'), X1)\n",
    "\n",
    "    for i in range(3):\n",
    "        if distribution == 'normal':\n",
    "            noise.normal_(0, 0.2)\n",
    "        else:\n",
    "            noise_np = np.random.beta(train_config['z_beta_a'],\n",
    "                                      train_config['z_beta_b'],\n",
    "                                      noise.shape)\n",
    "            noise = torch.tensor(noise_np).float().round().to(device)\n",
    "        with torch.no_grad():\n",
    "            X_g = G(noise)\n",
    "        if X_g.shape[-2:] == (3, 2048):\n",
    "            X_g.transpose_(1, 2)\n",
    "        X_g=X_g.cpu()\n",
    "        np.save(join(train_results_path, 'results', f'{epoch:05}_Xg_{i}'), X_g)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_e = E(X.transpose(1, 2))\n",
    "        if isinstance(z_e, tuple):\n",
    "            z_e = z_e[0]\n",
    "        X_rec = G(z_e)\n",
    "    if X_rec.shape[-2:] == (3, 2048):\n",
    "        X_rec.transpose_(1, 2)\n",
    "    X_rec1=X_rec.cpu()\n",
    "    np.save(join(train_results_path, 'results', f'{epoch:05}_Xrec'), X_rec1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5a9393d-595c-447f-a238-91d59784fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 18:10:43,984: DEBUG    Starting from epoch: 910\n",
      "2021-08-11 18:10:43,984: DEBUG    Starting from epoch: 910\n",
      "2021-08-11 18:10:43,984: DEBUG    Starting from epoch: 910\n",
      "2021-08-11 18:10:43,984: DEBUG    Starting from epoch: 910\n",
      "2021-08-11 18:10:43,984: DEBUG    Starting from epoch: 910\n",
      "2021-08-11 18:10:43,984: DEBUG    Starting from epoch: 910\n",
      "2021-08-11 18:10:43,984: DEBUG    Starting from epoch: 910\n",
      "2021-08-11 18:10:43,984: DEBUG    Starting from epoch: 910\n",
      "2021-08-11 18:10:43,991: DEBUG    Device variable: cuda\n",
      "2021-08-11 18:10:43,991: DEBUG    Device variable: cuda\n",
      "2021-08-11 18:10:43,991: DEBUG    Device variable: cuda\n",
      "2021-08-11 18:10:43,991: DEBUG    Device variable: cuda\n",
      "2021-08-11 18:10:43,991: DEBUG    Device variable: cuda\n",
      "2021-08-11 18:10:43,991: DEBUG    Device variable: cuda\n",
      "2021-08-11 18:10:43,991: DEBUG    Device variable: cuda\n",
      "2021-08-11 18:10:43,991: DEBUG    Device variable: cuda\n",
      "2021-08-11 18:10:43,999: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 18:10:43,999: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 18:10:43,999: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 18:10:43,999: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 18:10:43,999: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 18:10:43,999: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 18:10:43,999: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 18:10:43,999: DEBUG    Current CUDA device: 0\n",
      "2021-08-11 18:10:44,058: DEBUG    Selected all classes. Loaded 1601 samples.\n",
      "2021-08-11 18:10:44,058: DEBUG    Selected all classes. Loaded 1601 samples.\n",
      "2021-08-11 18:10:44,058: DEBUG    Selected all classes. Loaded 1601 samples.\n",
      "2021-08-11 18:10:44,058: DEBUG    Selected all classes. Loaded 1601 samples.\n",
      "2021-08-11 18:10:44,058: DEBUG    Selected all classes. Loaded 1601 samples.\n",
      "2021-08-11 18:10:44,058: DEBUG    Selected all classes. Loaded 1601 samples.\n",
      "2021-08-11 18:10:44,058: DEBUG    Selected all classes. Loaded 1601 samples.\n",
      "2021-08-11 18:10:44,058: DEBUG    Selected all classes. Loaded 1601 samples.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-c', '--config', default=None, type=str,\n",
    "#                         help='File path for evaluation config')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    evaluation_config = None\n",
    "#     if args.config is not None and args.config.endswith('.json'):\n",
    "    with open(\"hyperparams.json\") as f:\n",
    "        evaluation_config = json.load(f)\n",
    "    assert evaluation_config is not None\n",
    "\n",
    "    main(evaluation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d48f05f-19e1-4703-b4ec-67476a72a1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/3DAEE'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28e6c4f-b7b5-4af1-96a9-1947d26d04c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/3DAEE'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e2f3e-baca-4d89-b985-74c8d37f033c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3DAAE",
   "language": "python",
   "name": "daiaaae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
